{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "random.seed(141)\n",
    "np.random.seed(141)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStateMDP:\n",
    "    def __init__(self):\n",
    "        self.num_states = 2\n",
    "        self.v_actions = [[0,1], [2]] # eligible actions: state0 -> {0 and 1}, state1 ->{2}\n",
    "\n",
    "        flat_acList = [item for sublist in self.v_actions for item in sublist] # flatten the list of lists\n",
    "        self.i_numActions = len(set(flat_acList))\n",
    "\n",
    "        # transition probabilities\n",
    "        self.v_trp = [[[0 for i in range(self.num_states)] for j in range(self.num_states)] for k in range(self.i_numActions)]\n",
    "\n",
    "        self.v_trp[0][0][0] = 0.25\n",
    "        self.v_trp[0][0][1] = 0.75\n",
    "\n",
    "        self.v_trp[1][0][0] = 0\n",
    "        self.v_trp[1][0][1] = 1.0\n",
    "\n",
    "        self.v_trp[2][1][0] = 0\n",
    "        self.v_trp[2][1][1] = 1.0\n",
    "\n",
    "        # rewards\n",
    "        self.v_rews = [[0 for i in range(self.i_numActions)] for j in range(self.num_states)]\n",
    "        self.v_rews[0][0] = 5\n",
    "        self.v_rews[0][1] = 10\n",
    "        self.v_rews[1][2] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.25, 0.75], [0, 0]], [[0, 1.0], [0, 0]], [[0, 0], [0, 1.0]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdpEx = TwoStateMDP()\n",
    "mdpEx.v_trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InventoryManagement:\n",
    "    def __init__(self):\n",
    "        # Mapping of states to indices\n",
    "        self.state_mapping = {'full': 0, 'medium': 1, 'low': 2, 'empty': 3}\n",
    "        # Mapping of actions to indices\n",
    "        self.action_mapping = {'NO': 0, 'OS': 1, 'OM': 2, 'OL': 3}\n",
    "        # Number of states and actions\n",
    "        self.num_states = len(self.state_mapping)\n",
    "        self.num_actions = len(self.action_mapping)\n",
    "        self.v_actions = [[action for action in range(self.num_actions)] for _ in range(self.num_states)]\n",
    "\n",
    "        # Initialize transition probabilities and rewards matrices\n",
    "        self.trp = [[[0.0 for _ in range(self.num_states)] for _ in range(self.num_states)] for _ in range(self.num_actions)]\n",
    "        self.rews3D = [[[0 for _ in range(self.num_states)] for _ in range(self.num_states)] for _ in range(self.num_actions)]\n",
    "        # Initialize rewards for actions in all states\n",
    "        self.rews = [[0 for _ in range(self.num_actions)] for _ in range(self.num_states)]\n",
    "        \n",
    "    def load_transitions_and_rewards(self, datafile):\n",
    "        data = pd.read_csv(datafile)\n",
    "        # Loop over each row in the DataFrame to populate the matrices\n",
    "        for _, row in data.iterrows():\n",
    "            # Get the state, action, and next state indices\n",
    "            s_idx = self.state_mapping[row['s']]\n",
    "            a_idx = self.action_mapping[row['a']]\n",
    "            s_next_idx = self.state_mapping[row['s_next']]\n",
    "            # Populate the transition probability\n",
    "            self.trp[a_idx][s_idx][s_next_idx] = row['p(s_next|s,a)']\n",
    "            # Populate the reward\n",
    "            self.rews3D[a_idx][s_idx][s_next_idx] = row['r(s,a,s_next)']\n",
    "        \n",
    "        # Calculate the expected rewards for actions in all states\n",
    "        for a in range(self.num_actions):\n",
    "            for s in range(self.num_states):\n",
    "                # Expected reward is the sum of rewards weighted by their probabilities\n",
    "                self.rews[s][a] = sum(self.trp[a][s][ss] * self.rews3D[a][s][ss] for ss in range(self.num_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]],\n",
       " [[[0.2, 0.8, 0.0, 0.0],\n",
       "   [0.0, 0.3, 0.6, 0.1],\n",
       "   [0.0, 0.0, 0.0, 1.0],\n",
       "   [0.0, 0.0, 0.0, 1.0]],\n",
       "  [[1.0, 0.0, 0.0, 0.0],\n",
       "   [0.7, 0.3, 0.0, 0.0],\n",
       "   [0.0, 1.0, 0.0, 0.0],\n",
       "   [0.0, 0.0, 1.0, 0.0]],\n",
       "  [[1.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 0.0, 0.0, 0.0],\n",
       "   [0.8, 0.2, 0.0, 0.0],\n",
       "   [0.0, 1.0, 0.0, 0.0]],\n",
       "  [[1.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 0.0, 0.0, 0.0]]],\n",
       " [[[5, 5, 0, 0], [0, 2, 2, 2], [0, 0, 0, -10], [0, 0, 0, -20]],\n",
       "  [[-2, 0, 0, 0], [4, 4, 0, 0], [0, 0, 0, 0], [0, 0, -15, 0]],\n",
       "  [[-2, 0, 0, 0], [-1, 0, 0, 0], [10, 10, 0, 0], [0, -5, 0, 0]],\n",
       "  [[-2, 0, 0, 0], [-1, 0, 0, 0], [-1, 0, 0, 0], [15, 0, 0, 0]]],\n",
       " [[5.0, -2.0, -2.0, -2.0],\n",
       "  [1.9999999999999998, 4.0, -1.0, -1.0],\n",
       "  [-10.0, 0.0, 10.0, -1.0],\n",
       "  [-20.0, -15.0, -5.0, 15.0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the class\n",
    "invEx = InventoryManagement()\n",
    "\n",
    "# Load the transitions and rewards from the CSV file data\n",
    "invEx.load_transitions_and_rewards(\"invData.csv\")\n",
    "\n",
    "# Check if the loading was done correctly\n",
    "invEx.v_actions, invEx.trp, invEx.rews3D, invEx.rews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyEval_VI(giNumStates, gvPol, gvTrp, gvRews, gdDiscountFactor=0.9):\n",
    "    \n",
    "    print(\"Policy\",gvPol)\n",
    "\n",
    "    t_dEpsilon = 0.0001\n",
    "    tmpv_vals = [0]*giNumStates\n",
    "    \n",
    "    t_iCtr = 0\n",
    "    while True:\n",
    "        # print(\"t_iCtr\", t_iCtr)\n",
    "        # print(\"tmpv_vals\", tmpv_vals)\n",
    "        currentv_vals = [0]*giNumStates\n",
    "        for s in range(giNumStates):\n",
    "            \n",
    "            a = gvPol[s]\n",
    "            currentv_vals[s] = gvRews[s][a]\n",
    "\n",
    "            for ss in range(giNumStates):\n",
    "                currentv_vals[s] += gdDiscountFactor*gvTrp[a][s][ss]*tmpv_vals[ss]\n",
    "        \n",
    "        currentv_vals = np.around(currentv_vals, 3)\n",
    "        # print(\"currentv_vals\", currentv_vals)\n",
    "        if np.linalg.norm(np.array(tmpv_vals) - np.array(currentv_vals)) < t_dEpsilon:\n",
    "            break\n",
    "        \n",
    "        tmpv_vals = currentv_vals\n",
    "        t_iCtr = t_iCtr + 1\n",
    "\n",
    "    return tmpv_vals, t_iCtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy [0, 2]\n",
      "\n",
      "values at each state\n",
      "V( 0 ) -2.254\n",
      "V( 1 ) -9.996\n"
     ]
    }
   ],
   "source": [
    "v_vals,_ = PolicyEval_VI(mdpEx.num_states, [0,2], mdpEx.v_trp, mdpEx.v_rews, gdDiscountFactor=0.9)\n",
    "\n",
    "print('\\nvalues at each state')\n",
    "for i in range(len(v_vals)):\n",
    "    print(\"V(\",i,\")\", v_vals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy [3, 3, 3, 3]\n",
      "\n",
      "values at each state\n",
      "V( 0 ) -19.996\n",
      "V( 1 ) -18.996\n",
      "V( 2 ) -18.996\n",
      "V( 3 ) -2.996\n"
     ]
    }
   ],
   "source": [
    "# inventory ex: evaluate the policy of always order large (OL)\n",
    "v_vals,_ = PolicyEval_VI(invEx.num_states, [3,3,3,3], invEx.trp, invEx.rews, gdDiscountFactor=0.9)\n",
    "\n",
    "print('\\nvalues at each state')\n",
    "for i in range(len(v_vals)):\n",
    "    print(\"V(\",i,\")\", v_vals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy [0, 1, 2, 3]\n",
      "\n",
      "values at each state\n",
      "V( 0 ) 45.032\n",
      "V( 1 ) 44.342\n",
      "V( 2 ) 50.405\n",
      "V( 3 ) 55.529\n"
     ]
    }
   ],
   "source": [
    "# inventory ex: evaluate the policy of ordering based on inventory level, e.g., OL when empty, NO when full\n",
    "v_vals,_ = PolicyEval_VI(invEx.num_states, [0,1,2,3], invEx.trp, invEx.rews, gdDiscountFactor=0.9)\n",
    "\n",
    "print('\\nvalues at each state')\n",
    "for i in range(len(v_vals)):\n",
    "    print(\"V(\",i,\")\", v_vals[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(giNumStates, gvActions, gvTrp, gvRews, gdDiscountFactor=0.9):\n",
    "    \n",
    "    t_dEpsilon = 0.000001\n",
    "    tmpv_vals = [0]*giNumStates\n",
    "\n",
    "    flat_list = [item for sublist in gvActions for item in sublist] # flatten the list of lists\n",
    "    t_iNumActions = len(set(flat_list))\n",
    "    \n",
    "    t_iCtr = 0\n",
    "    while True:\n",
    "        \n",
    "        # print(\"t_iCtr\", t_iCtr)\n",
    "        # print(\"tmpv_vals\", tmpv_vals)\n",
    "        \n",
    "        currentv_vals = [0]*giNumStates\n",
    "        t_qValues = [[float(\"-inf\") for a in range(t_iNumActions)] for s in range(giNumStates)]\n",
    "        t_vPol = [float(\"-inf\") for s in range(giNumStates)]\n",
    "        for s in range(giNumStates):\n",
    "            for a in gvActions[s]:\n",
    "                t_qValues[s][a] = gvRews[s][a]\n",
    "                for ss in range(giNumStates):\n",
    "                    t_qValues[s][a] += gdDiscountFactor*gvTrp[a][s][ss]*tmpv_vals[ss]\n",
    "        \n",
    "        # print(\"t_qValues\", t_qValues)\n",
    "        currentv_vals = np.amax(np.around(t_qValues, 3), axis = 1)\n",
    "        t_vPol = np.argmax(np.around(t_qValues, 3), axis = 1)\n",
    "        # print(\"currentv_vals\", currentv_vals)\n",
    "        if np.linalg.norm(np.array(tmpv_vals) - np.array(currentv_vals)) < t_dEpsilon:\n",
    "            break\n",
    "        \n",
    "        tmpv_vals = currentv_vals\n",
    "        t_iCtr = t_iCtr + 1\n",
    "\n",
    "    return tmpv_vals, t_vPol, t_iCtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V( 0 ) 1.004\n",
      "V( 1 ) -9.996\n"
     ]
    }
   ],
   "source": [
    "v_vals, t_vPol, _ = ValueIteration(mdpEx.num_states, mdpEx.v_actions, mdpEx.v_trp, mdpEx.v_rews, gdDiscountFactor=0.9)\n",
    "for i in range(len(v_vals)):\n",
    "    print(\"V(\",i,\")\", v_vals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values-policy:\n",
      "\n",
      "(52.39, 0)\n",
      "(52.722, 0)\n",
      "(57.211, 2)\n",
      "(62.151, 3)\n"
     ]
    }
   ],
   "source": [
    "v_vals, v_pol, _ = ValueIteration(invEx.num_states, invEx.v_actions, invEx.trp, invEx.rews, gdDiscountFactor=0.9)\n",
    "print(\"\\nvalues-policy:\\n\")\n",
    "for val in zip(v_vals, v_pol):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(num_states, num_actions, v_actions, v_trp, v_rews, episodes = 100, runs=10, discount_factor=0.9, step_size=0.5):\n",
    "    iter_limit = 100\n",
    "    epsilon = 0.05  # randomness in action selection\n",
    "\n",
    "    rewards_q_learning = np.zeros(episodes)\n",
    "    for r in tqdm(range(runs)):\n",
    "        # initialize Q-values\n",
    "        q_value = np.full((num_states, num_actions), -np.inf)  # Use negative infinity for impossible actions\n",
    "        for s in range(num_states):\n",
    "            for a in v_actions[s]:\n",
    "                q_value[s, a] = 0.0  # Initialize eligible actions to zero\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = random.choice(range(num_states))  # Start at a random state\n",
    "            rewards = 0.0\n",
    "\n",
    "            for iter_ctr in range(iter_limit):\n",
    "                # Epsilon-greedy action selection\n",
    "                if random.random() < epsilon:\n",
    "                    action = np.random.choice(v_actions[state])\n",
    "                else:\n",
    "                    # Select one of the actions with the highest Q-value\n",
    "                    max_q_value = np.max(q_value[state, v_actions[state]])\n",
    "                    best_actions = [a for a in v_actions[state] if q_value[state, a] == max_q_value]\n",
    "                    action = np.random.choice(best_actions)\n",
    "\n",
    "                # Transition to next state and receive reward\n",
    "                # Here we assume that the transition probabilities for each action are stored in a way that corresponds to the state indices.\n",
    "                next_state = np.random.choice(np.arange(num_states), p=v_trp[action][state])\n",
    "                reward = v_rews[state][action]\n",
    "                rewards += reward\n",
    "\n",
    "                # Q-Learning update\n",
    "                q_value[state, action] += step_size * (reward + discount_factor * np.max(q_value[next_state, v_actions[next_state]]) - q_value[state, action])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            rewards_q_learning[i] += rewards\n",
    "\n",
    "    rewards_q_learning /= runs\n",
    "    plt.plot(rewards_q_learning, label='Q-Learning')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Sum of rewards during episode')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Derive the optimal policy from Q-values\n",
    "    optimal_policy = {s: np.argmax(q_value[s, :]) for s in range(num_states) if not np.all(q_value[s, :] == -np.inf)}\n",
    "\n",
    "    return optimal_policy, q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values-policy:\n",
      "\n",
      "(1.0000000000000193, 0)\n",
      "(-9.999999999999979, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/q8/jq2qqhms2y18nl_r3vs5007r0000gn/T/ipykernel_9076/649298873.py:45: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, q_value = QLearning(mdpEx.num_states, mdpEx.i_numActions, mdpEx.v_actions, mdpEx.v_trp, mdpEx.v_rews)\n",
    "\n",
    "print(\"\\nvalues-policy:\\n\")\n",
    "for val in zip(np.max(q_value,1), optimal_policy):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values-policy:\n",
      "\n",
      "(54.348122687990305, 0)\n",
      "(55.01744333343609, 1)\n",
      "(59.17852384051515, 2)\n",
      "(64.25363273966377, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/q8/jq2qqhms2y18nl_r3vs5007r0000gn/T/ipykernel_9076/649298873.py:45: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, q_value = QLearning(invEx.num_states, invEx.num_actions, invEx.v_actions, invEx.trp, invEx.rews, episodes = 1000, runs=10)\n",
    "\n",
    "print(\"\\nvalues-policy:\\n\")\n",
    "for val in zip(np.max(q_value,1), optimal_policy):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envp3t)",
   "language": "python",
   "name": "envp3t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
